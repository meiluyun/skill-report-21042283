{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    \n",
    "from math import log  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):   \n",
    "\n",
    "    returnVec = [0] * len(vocabList)  \n",
    "    for word in inputSet:          \n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1   \n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](垃圾邮件.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainNB0(trainMatrix, trainCategory): \n",
    "    \n",
    "\n",
    "    numTrainDocs = len(trainMatrix)                      \n",
    "    numWords = len(trainMatrix[0])                       \n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)  \n",
    "    p0Num = np.ones(numWords)                           \n",
    "    p1Num = np.ones(numWords)                            \n",
    "    p0Denom = 2.0  \n",
    "    p1Denom = 2.0  \n",
    "   \n",
    "    for i in range(numTrainDocs):   \n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num   += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "\n",
    "\n",
    "    print('Number of occurrences of all words in 0 category：',p0Denom) \n",
    "    print('Number of occurrences of all words in 1 category：',p1Denom)\n",
    "\n",
    "    p0Vect = np.log10(p0Num / p0Denom)  \n",
    "    p1Vect = np.log10(p1Num / p1Denom)  \n",
    "\n",
    "    print('log probability value corresponding to each word in the 0 category：',p0Vect)\n",
    "    print('log probability value corresponding to each word in the 1 category：',p1Vect)\n",
    "    print(pAbusive)\n",
    "\n",
    "    return p0Vect, p1Vect, pAbusive  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)  \n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X = [\n",
    "    ['He', 'took', 'his', 'dog', 'to', 'the', 'seaside'],                       \n",
    "    ['We', 'often', 'play', 'in', 'the', 'forest', 'in', 'summer'],            \n",
    "    ['if', 'this', 'is', 'not', 'love', 'then', 'what', 'is', 'love'],\n",
    "    ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "    ['Let', 'us', 'play', 'basketball', 'after', 'school', 'this', 'afternoon'],\n",
    "    ['We', 'should', 'love', 'our', 'father', 'and', 'mother', 'forever'],\n",
    "    ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "    ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid'],\n",
    "    ['Guess', 'which', 'team', 'will', 'be', 'the', 'NBA', 'champion', 'this', 'year'],\n",
    "    ['This', 'year', 'Lakers', 'should', 'win', 'the', 'championship']\n",
    "]\n",
    "Train_Y = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]  # 1:Insulting words，0:Normal words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'seaside', 'worthless', 'stupid', 'This', 'year', 'our', 'buying', 'play', 'his', 'which', 'love', 'if', 'posting', 'quit', 'should', 'and', 'summer', 'him', 'Let', 'food', 'garbage', 'us', 'this', 'ate', 'forest', 'what', 'my', 'Guess', 'after', 'Lakers', 'win', 'took', 'forever', 'often', 'be', 'to', 'father', 'steak', 'He', 'mother', 'We', 'school', 'team', 'stop', 'afternoon', 'champion', 'basketball', 'is', 'NBA', 'dog', 'will', 'championship', 'in', 'how', 'licks', 'then', 'the', 'mr']\n",
      "Number of words in non repeating array: 59\n"
     ]
    }
   ],
   "source": [
    "myVocabSet = set([])       \n",
    "\n",
    "for document in Train_X: \n",
    "    myVocabSet = myVocabSet | set(document)  \n",
    "myVocabList=list(myVocabSet)\n",
    "\n",
    "print(myVocabList)\n",
    "print('Number of words in non repeating array:',len(myVocabList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is done for each row of samples\n",
      "[0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0]\n",
      "Feature set dimension： (10, 59)\n",
      "Target set dimension： (10,)\n"
     ]
    }
   ],
   "source": [
    "trainMat = []  \n",
    "for traini in Train_X:  \n",
    "    trainMat.append(setOfWords2Vec(myVocabList, traini))  \n",
    "\n",
    "trainMat=np.array(trainMat)\n",
    "Train_Y=np.array(Train_Y)\n",
    "print(\"This is done for each row of samples\")\n",
    "\n",
    "print(trainMat[0])\n",
    "\n",
    "print('Feature set dimension：',trainMat.shape)\n",
    "print('Target set dimension：',Train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of occurrences of all words in 0 category： 43.0\n",
      "Number of occurrences of all words in 1 category： 35.0\n",
      "log probability value corresponding to each word in the 0 category： [-1.33243846 -1.33243846 -1.63346846 -1.63346846 -1.63346846 -1.33243846\n",
      " -1.63346846 -1.63346846 -1.33243846 -1.33243846 -1.33243846 -1.33243846\n",
      " -1.33243846 -1.63346846 -1.63346846 -1.63346846 -1.63346846 -1.63346846\n",
      " -1.33243846 -1.33243846 -1.63346846 -1.63346846 -1.33243846 -1.03140846\n",
      " -1.33243846 -1.63346846 -1.33243846 -1.33243846 -1.33243846 -1.33243846\n",
      " -1.63346846 -1.63346846 -1.33243846 -1.63346846 -1.63346846 -1.33243846\n",
      " -1.1563472  -1.63346846 -1.33243846 -1.33243846 -1.63346846 -1.63346846\n",
      " -1.33243846 -1.33243846 -1.33243846 -1.33243846 -1.33243846 -1.33243846\n",
      " -1.33243846 -1.33243846 -1.33243846 -1.33243846 -1.63346846 -1.63346846\n",
      " -1.33243846 -1.33243846 -1.33243846 -1.1563472  -1.33243846]\n",
      "log probability value corresponding to each word in the 1 category： [-1.54406804 -1.54406804 -1.06694679 -1.06694679 -1.24303805 -1.24303805\n",
      " -1.24303805 -1.24303805 -1.24303805 -1.54406804 -1.54406804 -1.24303805\n",
      " -1.54406804 -1.24303805 -1.24303805 -1.06694679 -1.24303805 -1.24303805\n",
      " -1.54406804 -1.54406804 -1.24303805 -1.24303805 -1.54406804 -1.54406804\n",
      " -1.54406804 -1.24303805 -1.54406804 -1.54406804 -1.54406804 -1.54406804\n",
      " -1.24303805 -1.24303805 -1.54406804 -1.24303805 -1.24303805 -1.54406804\n",
      " -1.54406804 -1.24303805 -1.54406804 -1.54406804 -1.24303805 -1.06694679\n",
      " -1.54406804 -1.54406804 -1.24303805 -1.54406804 -1.54406804 -1.54406804\n",
      " -1.54406804 -1.54406804 -1.24303805 -1.54406804 -1.24303805 -1.24303805\n",
      " -1.54406804 -1.54406804 -1.54406804 -1.06694679 -1.54406804]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "p0V, p1V, pAb = trainNB0(trainMat, Train_Y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'then'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testEntry = ['love', 'my', 'then']\n",
    "thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))\n",
    "\n",
    "testEntry = ['stupid', 'garbage']\n",
    "thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
